---
title: Using DeepSeek-R1 Model
description: "Use the DeepSeek-R1 Model with your Modus app"
mode: "wide"
"og:title": "Using DeepSeek - Modus"
---

`DeepSeek-R1` is an open source AI reasoning model that rivals the performance
of frontier models such as OpenAI's o1 in complex reasoning tasks like math and
coding. Benefits of DeepSeek include:

- **Performance**: `DeepSeek-R1` achieves comparable results to OpenAI's o1
  model on several benchmarks.
- **Efficiency**: The model uses significantly fewer parameters and therefore
  operates at a lower costs relative to competing frontier models.
- **Open Source**: The open source license allows both commercial and
  non-commercial usage of the model weights and associated code.
- **Novel training approach**: The research team developed DeepSeek-R1 through a
  multi-stage approach that combines reinforcement learning, fine-tuning, and
  data distillation. distillation.
- **Distilled versions**: The DeepSeek team released smaller, distilled models
  based on DeepSeek-R1 that offer high reasoning capabilities with fewer
  parameters.

In this guide we review how to use the `DeepSeek-R1` model in your Modus app.

## Options for using DeepSeek with Modus

There are two options for invoking `DeepSeek-R1` in your Modus app:

1. [Use the distilled `DeepSeek-R1-Distill-Llama-8B` model hosted by Hypermode](#using-the-distilled-deepseek-model-hosted-by-hypermode)
   Hypermode hosts and makes available the distilled DeepSeek model based on
   Llama-3.1-8B, enabling Modus apps to use it in both local development
   environments and deployed applications.
2. [Use the DeepSeek API with your Modus app](#using-the-deepseek-api-with-modus)
   Access DeepSeek models hosted on the DeepSeek platform by configuring a
   DeepSeek connection in your Modus app and using your DeepSeek API key

## Using the distilled DeepSeek model hosted by Hypermode

The open source `DeepSeek-R1-Distill-Llama-8B` DeepSeek model is available on
Hypermode as a shared model. This means that we can invoke this model in a Modus
app in both a local development environment and also in an app deployed on
Hypermode.

The `DeepSeek-R1-Distill-Llama-8B` is a distilled version of the DeepSeek-R1
model which has been fine-tuned using the `Llama-3.1-8B` model as a base model,
using samples generated by DeepSeek-R1.

Distilled models offer similar high reasoning capabilities with fewer
parameters.

<Steps>

<Step title="Create a Modus app">

If you haven't already, create a new modus app. Skip this step if you already
have a Modus app.

```sh
modus new
```

<Tip>See [this page]() for more information about creating Modus projects</Tip>

</Step>

<Step title="Add the DeepSeek model to your app manifest">

Update the `modus.json` app manifest file to specify the
`DeepSeek-R1-Distill-Llama-8B` model hosted on Hypermode.

```json modus.json
  "models": {
    "deepseek-r1-distill": {
      "sourceModel": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "provider": "hugging-face",
      "connection": "hypermode"
    }
  },
```

<Note>
  Note that we named the model `deepseek-r1-distill` in our app manifest, which
  we use to access the model in our Modus function.
</Note>

</Step>

<Step title="Use the hyp CLI to sign in to Hypermode">

To use Hypermode models locally we must use the `hyp` CLI to log in to
Hypermode.

Install the `hyp` CLI if not previously installed.

```sh
npm i -g @hypermode/hyp-cli
```

Login to Hypermode using this command:

```sh
hyp login
```

This

</Step>

<Step title="Write a function to invoke the model">

Now the `DeepSeek-R1-Distill-Llama-8B` model can be used in our Modus app. The
following function takes a user prompt as a parameter and returns the generated
text, however the model can also be used for more advanced workflows such as
returning structured data or in a tool use / function

</Step>

<Step title="Query your function in the Modus API Explorer">

TODO: screen shot

<Tip>
  For mathematical problems, it is advisable to include a directive in your
  prompt such as: "Please reason step by step, and put your final answer within
  > \boxed{}."
</Tip>

</Step>

</Steps>

## Using the DeepSeek API with Modus

<Steps>
<Step title="Create a DeepSeek API token">
TODO: screen shot of DeepSeek console
</Step>
<Step title="Create a Modus app">
If you haven't already, create a new modus app. Skip this step if you already have a Modus app.

```sh
modus new
```

See [this doc]() for more information about creating Modus projects

</Step>

<Step title="Define the model and connection in your app manifest">

```json modus.json
{
  "$schema": "https://schema.hypermode.com/modus.json",
  "endpoints": {
    "default": {
      "type": "graphql",
      "path": "/graphql",
      "auth": "bearer-token"
    }
  },
  "models": {
    "deepseek-reasoner": {
      "sourceModel": "deepseek-reasoner",
      "connection": "deepseek",
      "path": "v1/chat/completions"
    }
  },
  "connections": {
    "deepseek": {
      "type": "http",
      "baseUrl": "https://api.deepseek.com/",
      "headers": {
        "Authorization": "Bearer {{API_TOKEN}}"
      }
    }
  }
}
```

</Step>

<Step title="Create environment variable for your API token">
Create a `.env` file or edit the `.env.dev.local` file

```env
MODUS_DEEPSEEK_API_TOKEN=<YOUR_TOKEN_VALUE_HERE>
```

</Step>

<Step>

<CodeGroup>

    ```go Go
    package main

    import (
        "strings"

        "github.com/hypermodeinc/modus/sdk/go/pkg/models"
        "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
    )

    func GenerateText(prompt string) (string, error) {

        model, err := models.GetModel[openai.ChatModel]("deepseek-reasoner")
        if err != nil {
            return "", err
        }

        // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
        input, err := model.CreateInput(
            openai.NewUserMessage(prompt),
        )
        if err != nil {
            return "", err
        }

        // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
        input.Temperature = 0.6

        // Here we invoke the model with the input we created.
        output, err := model.Invoke(input)
        if err != nil {
            return "", err
        }

        // The output is also specific to the ChatModel interface.
        // Here we return the trimmed content of the first choice.
        return strings.TrimSpace(output.Choices[0].Message.Content), nil
    }
    ```

```ts AssemblyScript
import { models } from "@hypermode/modus-sdk-as"

import {
  OpenAIChatModel,
  SystemMessage,
  UserMessage,
} from "@hypermode/modus-sdk-as/models/openai/chat"

export function generateText(prompt: string): string {
  const model = models.getModel<OpenAIChatModel>("deepseek-reasoner")

  // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
  const input = model.createInput([new UserMessage(prompt)])

  // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
  input.temperature = 0.6

  // Here we invoke the model with the input we created.
  const output = model.invoke(input)

  // The output is also specific to the OpenAIChatModel interface.
  // Here we return the trimmed content of the first choice.
  return output.choices[0].message.content.trim()
}
```

</CodeGroup>

</Step>

<Step title="Run your Modus app">

Run `npm run dev`

this will compile your Modus app and start a local GraphQL API

</Step>

<Step title="Query using the Modus API Explorer">
  TODO: screen shot, query / results

<Tip>
  For mathematical problems, it is advisable to include a directive in your
  prompt such as: "Please reason step by step, and put your final answer within
  > \boxed{}."
</Tip>

</Step>

</Steps>

## Resources

- DeepSeek on HuggingFace
- DeepSeek-R1-Distill-Llama-8B model card
- DeekSeek paper
- DeekSeek Platform API docs
